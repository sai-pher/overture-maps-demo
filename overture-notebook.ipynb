{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overture Admin Areas Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Overture Maps Foundation](https://overturemaps.org) recently published its first-release version of open map data 2023-07-26-alpha.0. Included in this is the Administrative Boundaries Theme, which contains the geometries of Administrative Areas down to the state level (admin_level 2 & 4). Administrative area polygons form the basis for what we often consider to be a \"world map\" and contribute to effective spatial representation, analysis, and decision-making.\n",
    "\n",
    "Administrative areas are used as the backbone for demographic profiling, service accessibility assessments and determining where important rescources are located and should be allocated. Visualizing administrative areas on a digital map makes complex demographic divisions more understandable to a wide audience. This can aid communication and decision-making for governance, economics, public health, urban planning, environmental management, and more.\n",
    "\n",
    "In this tutorial, we will look at how to:\n",
    "\n",
    "1. Read Admin data from Overture Maps.\n",
    "2. Make spatial queries on the admin data.\n",
    "3. Combine the data with external open source data.\n",
    "4. Display the geospatial result for anlysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Admin data from Overture Maps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overture Maps data is stored in Azure blob storage and AWS S3 buckets using parkets. Details on the map schema and where to get the official data can be found [here](https://overturemaps.org/download/).\n",
    "\n",
    "To read this data, we will use duckdb to read the parquet files. This uses the example query provided [here](https://github.com/OvertureMaps/data/blob/main/README.md#3-duckdb-sql)\n",
    "\n",
    "Note: The full dataset is `~1.2Gb`. It only needs to be downloaded once so make sure to only run this secion of the tutorial for the initial download step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "db = duckdb.connect()\n",
    "db.execute(\"INSTALL spatial\")\n",
    "db.execute(\"INSTALL httpfs\")\n",
    "db.execute(\"\"\"\n",
    "LOAD spatial;\n",
    "LOAD httpfs;\n",
    "SET s3_region='us-west-2';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(\"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "           type,\n",
    "           subType,\n",
    "           localityType,\n",
    "           adminLevel,\n",
    "           isoCountryCodeAlpha2,\n",
    "           JSON(names) AS names,\n",
    "           JSON(sources) AS sources,\n",
    "           ST_GeomFromWkb(geometry) AS geometry\n",
    "      FROM read_parquet('s3://overturemaps-us-west-2/release/2023-07-26-alpha.0/theme=admins/type=*/*', filename=true, hive_partitioning=1)\n",
    "     WHERE adminLevel in (2, 4)\n",
    "       AND ST_GeometryType(ST_GeomFromWkb(geometry)) IN ('POLYGON','MULTIPOLYGON')\n",
    ") TO 'data/admin.geojson'\n",
    "WITH (FORMAT GDAL, DRIVER 'GeoJSON');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into a usebale format using Geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data downloaded, we can use geopandas to read the file using `.read_file()`. This takes a string path to the file location, and reads it into a geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath\n",
    "fp = \"data/admin.geojson\"\n",
    "\n",
    "# Read the file into a dataframe\n",
    "admin_data_df = gpd.read_file(fp)\n",
    "\n",
    "# Take a look at the data\n",
    "admin_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = admin_data_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a dataframe, we can use pandas and geopandas API's to query and manipulate it into differnt shapes that we would like.\n",
    "\n",
    "Let's practice this by extracting the local and english names of all the features in the `admin_data_df` dataframe, and adding them as a new column to the dataframe.\n",
    "\n",
    "Names for Overture admin features are added by TomTom, and enriched using ESRI datasets. To make the data useable in as many countries as possible, these names are stored as a JSON object in the data to allow users the flexibilty to select the names they would like to use for thier application. The `local` name is the name of the feauter in the official language of the country (where applicable) and can be considered to be the features \"defualt\" name, as recognised by the citizens of that country.\n",
    "\n",
    "To extract and append these names to the dataframe, we will use a function that reads the names JSON column, and extracts the values where language is `local` and `en` into two new columns `localname` and `enname`. This function will then be apllied to each row of the dataframe usineg the `.apply()` pandas function, with the `axis=1` input parameter to specify that we want this function to be applied row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_local_and_en_names(row):\n",
    "    # read the names JSON list if available\n",
    "    names = row['names']['common'] if pd.notna(row['names']) else []\n",
    "\n",
    "    for name in names:\n",
    "        # append local name\n",
    "        if name['language'] == 'local':\n",
    "            row['localname'] = name['value']\n",
    "        \n",
    "        # append english name\n",
    "        if name['language'] == 'en' or name['language'] == 'en-Latn':\n",
    "            row['enname'] = name['value']\n",
    "    \n",
    "    return row\n",
    "\n",
    "admin_data_df = admin_data_df.apply(append_local_and_en_names, axis=1)\n",
    "\n",
    "admin_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = admin_data_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our new name columns, let's create two new subsets of our data, using only the colums we need. \n",
    "\n",
    "Here, we will use the pandas `.query()` function to select the rows where the `adminlevel` value is `2` for country, or `4` for states. These two dataframes will be used later to perform spatial queries. We will make this query on the columns we need from `admin_data_df`, leaving us with a dataframe that has only the columns and rows that satisfy these contions for each subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = admin_data_df[['localname','enname','localitytype','isocountrycodealpha2','adminlevel','geometry']].query(\"adminlevel == 2\")\n",
    "countries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = countries_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states_df = admin_data_df[['localname','enname','localitytype','isocountrycodealpha2','adminlevel','geometry']].query(\"adminlevel == 4\")\n",
    "states_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = states_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two subsets created, we can go ahead to view our data on a map! Let's start with `countries_df`. We expect to see a map of the planet that shows all the countries in the world. To do this, we will use the geopandas `.explore()` method to display our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NOTE: This is where we start coming up against some of the challenges of using geospatial data. Due to the sheer size of some of the features (countries are roughly 140Mb, states are ~1Gb) rendering this data can be fairly memory intensive. \n",
    "We can overcome this by managing the resolution of features using the geopandas `.simplify()` method. This takes a tollerace value in the units of the [Coordinate Reference Systems](https://docs.qgis.org/3.4/en/docs/gentle_gis_introduction/coordinate_reference_systems.html) (CRS) used by the geospatial data. Here, we use the `WGS 84` system, which uses `degree` as its unit of measurement.\n",
    "\n",
    "What does this all mean?\n",
    "\n",
    "If you would like to load the data for visual inspection, while keeping a good balance between your local rescources and the accuracy of the displayed data, you can set the resolution using the following guide.\n",
    "\n",
    "- Low resolution    : Easiest to load       -> `tolerance=0.1`\n",
    "- Medium resolution : A good balance        -> `tolerance=0.01`\n",
    "- High resolution   : Farily accurate       -> `tolerance=0.001`\n",
    "- Full resolution   : Exact representation  -> use `.explore()` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_simplified_df(df, column, tolerance, cmap='Set1'):\n",
    "    # Copy the data to a new df only for visualisation\n",
    "    simp_df = df.copy()\n",
    "\n",
    "    # Simplify the geometry, and reset the column to display\n",
    "    simp_df['simplegeom'] = simp_df.simplify(tolerance=tolerance)\n",
    "    simp_df.set_geometry('simplegeom', inplace=True)\n",
    "\n",
    "    # Display the data, and colour based on the specified column\n",
    "    simp_df.set_geometry('simplegeom', inplace=True)\n",
    "    return simp_df.explore(column=column, cmap=cmap)\n",
    "\n",
    "# Display country geometries.\n",
    "display_simplified_df(countries_df, 'isocountrycodealpha2', 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make spatial queries on the admin data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been able to read, manipulate and view our geospatial data. Now, we will go over how to make spatial queries to find the states of a given location. \n",
    "\n",
    "To do this, we will use a custom method that uses the geopandas `.sjoin()` method to find the states of a specified country, using the `countries_df` and `states_df` dataframes. We use the `within` predicate to search only for geometries that are contained within the given country geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_states(c_df, s_df, isoCode):\n",
    "    # find the specific country\n",
    "    country = c_df[c_df['isocountrycodealpha2'] == isoCode] \n",
    "    # Find the states in that country\n",
    "    states = s_df.sjoin(country[['isocountrycodealpha2','geometry']], predicate=\"within\")\n",
    "    # Drop uneeded columns\n",
    "    states.drop(columns=['isocountrycodealpha2_right','isocountrycodealpha2_left','index_right'], inplace=True)\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test this function by searching for the states in Ghana, using its ISO:alpha2 country-code `GH`.\n",
    "\n",
    "Note: We are using `.explore()` directly here because the scope of these geometries is relatively small, making a higher accuracy more desirable. However, if this operation is tasking on your development environment, change this to use the `display_simplified_df()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_states_df = get_country_states(c_df=countries_df, s_df=states_df, isoCode='GH')\n",
    "gh_states_df.explore('localname', cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data with external open source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have been able to read Overture maps data, display it and perform spatial queries on it, we can start using it with other available open source or proprierary data. \n",
    "\n",
    "In this example, we will use open source data from [Stats Bank Ghana](https://statsbank.statsghana.gov.gh) to try and answer a simple research question: Where are we most likely to find people who can use Overture Maps data.\n",
    "\n",
    "To answer this, we will use their \"Use of Internet on Laptop in Last 3 months of Population (6 years and older)\" [survey](https://statsbank.statsghana.gov.gh/pxweb/en/PHC%202021%20StatsBank/PHC%202021%20StatsBank__ICT/use_internet_on_device_3.px/). This survety contains the counts of people by Education, Locality, Geographic_Area, Sex and Age, who have used the internet on a laptop in the last 3 months. This should give us a rough estimate of how many poeple we may be able to find in a given region that can use this data.\n",
    "\n",
    "To do this, we will read in the data as a CSV, and store it in a pandas dataframe to simplify its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'data/ICT17.csv'\n",
    "ict17_df = pd.read_csv(fp)\n",
    "ict17_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = ict17_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is a very comprehensive dataset, with each category combination counted on a separate row. \n",
    "\n",
    "To use this, we will need to create a subset of the data with only the criteria we want to display. Let's start with the counts of people between the ages of 15 and 29 who have used the internet on a laptop in the last 3 months, and live in urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "young_urban_users_df = ict17_df[['Used_laptop', 'Education', 'Locality','Geographic_Area', 'Sex', '15-19', '20-24','25-29']].query(\"Used_laptop == 'Yes' and Locality == 'Urban' and Sex == 'Both sexes' and Education == 'Total'\")\n",
    "young_urban_users_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = young_urban_users_df.shape\n",
    "print(\"This dataframe has [{}] rows and [{}] columns\".format(shape[0], shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve this data by creating a new column for the total count of people ages 15-29, by using the `.sum()` pandas method on our three numerical columns, and adding up by row using `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "young_urban_users_df['15-29'] = young_urban_users_df[['15-19', '20-24','25-29']].sum(axis=1)\n",
    "young_urban_users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new dataframe, we can merge our geospatial data using the pandas `.merge()` function. This joins our two datasets on a shared column value to give us a combined dataset with the columns from both our geospatial, and survey datasets. We do this by setting `left_on='localname', right_on='Geographic_Area'`, which means that the data will be joined using the state names present in both of these columns from our left (geospatial) dataframe, and our right (survey) dataframe. \n",
    "\n",
    "Finally, the `how='left'` parameter is also set to ensure that only all the names present in our geospatial dataframe are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_states_and_data_df = pd.merge(left=gh_states_df, right=young_urban_users_df[['Geographic_Area', 'Locality', '15-29']], how='left', left_on='localname', right_on='Geographic_Area')\n",
    "gh_states_and_data_gdf = gpd.GeoDataFrame(gh_states_and_data_df)\n",
    "gh_states_and_data_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined data can now be displayed using the `.explore()` method. Here, we use the `BuPu` colour map on the `15-29` column, to give us a good contrast between low and high numbers, making it easier to spot the states we should be focusing on for this search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_states_and_data_gdf.explore(column='15-29', cmap='BuPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from this map, the two states we are most likely to find people that meet the search criteria are `Greater Accra` followed by `Ashanti`.\n",
    "\n",
    "We can also observe a grey hole in the `North East`. Given that we used all the names from our geospatial data, we can inffer that this is either not the exact name used for that region, or the survey did not capture that region. It could also be that that region was captured as a group of smaller regions that may fall in the Admin Level 6-8 levels. This will rewquire further data validation. For now though, we can use what we have to start making broad assessments.\n",
    "\n",
    "Let us tweak our search now to look for a differnt group. For examlpe, if we wanted to provide resources and support for women under the age of 25, living in rural areas, where would we be most likely to find them?\n",
    "\n",
    "Use the following section to refine your search criteria, and see what observations you can make with the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search criteria subset\n",
    "young_rural_female_users_df = ict17_df[['Used_laptop', 'Education', 'Locality','Geographic_Area', 'Sex', '15-19', '20-24']].query(\"Used_laptop == 'Yes' and Locality == 'Rural' and Sex == 'Female' and Education == 'Total'\")\n",
    "young_rural_female_users_df['15-24'] = young_rural_female_users_df[['15-19', '20-24']].sum(axis=1)\n",
    "\n",
    "# Merge the subset with the geospatial data\n",
    "gh_rural_female_users_by_state_df = pd.merge(left=gh_states_df, right=young_rural_female_users_df[['Geographic_Area', 'Locality', '15-24']], how='left', left_on='localname', right_on='Geographic_Area')\n",
    "gh_rural_female_users_by_state_gdf = gpd.GeoDataFrame(gh_rural_female_users_by_state_df)\n",
    "\n",
    "# Inspect the data\n",
    "gh_rural_female_users_by_state_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "gh_rural_female_users_by_state_gdf.explore(column='15-24', cmap='BuPu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
